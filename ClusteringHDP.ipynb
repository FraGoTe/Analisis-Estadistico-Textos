{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "ClusteringHDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FraGoTe/Analisis-Estadistico-Textos/blob/master/ClusteringHDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L7_9GlTAyPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#!rm -R dataset\n",
        "#!mkdir -p dataset\n",
        "#!wget https://raw.githubusercontent.com/githila/data/master/bbc.zip -P dataset\n",
        "\n",
        "#!unzip dataset/bbc.zip -d dataset/bbc\n",
        "#!rm dataset/bbc.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIom-1Qq_ggS",
        "colab_type": "text"
      },
      "source": [
        "# HDP (Hierarchical Dirichlet Process)\n",
        "\n",
        "## ¿Qué es?\n",
        "\n",
        "Es un algoritmo que se puede emplear para clustering de texto, realmente su funcionalidad es el modelado de temas, donde un tema es modelado a partir de grupo de documentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7toONtiz_ggW",
        "colab_type": "text"
      },
      "source": [
        "## Librerías requeridas\n",
        "\n",
        "- codecs\n",
        "- nltk\n",
        "- numpy\n",
        "- functools\n",
        "- re\n",
        "\n",
        "Las siguientes librerías son para leer múltiples archivos. El algoritmo base HDP no las necesita\n",
        "- os\n",
        "- pathlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uyf3miU_ggX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95WNgsr0_ggY",
        "colab_type": "code",
        "outputId": "29df392a-dd80-4769-85ca-1740fadafbba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk, re\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayE2W8MD_ggc",
        "colab_type": "text"
      },
      "source": [
        "### Stopwords\n",
        "\n",
        "Se define la lista de stopwords, en adición a las incluidas en _nltk.corpus.stopwords.words('english')_ list.\n",
        "Se genera un conjunto para no tener palabras repetidas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfBG_8B6_ggd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stopwords_list = nltk.corpus.stopwords.words('english')\n",
        "stopwords_list = \"a,s,able,about,above,according,accordingly,across,actually,after,afterwards,again,against,ain,t,all,allow,allows,almost,alone,along,already,also,although,always,am,among,amongst,an,and,another,any,anybody,anyhow,anyone,anything,anyway,anyways,anywhere,apart,appear,appreciate,appropriate,are,aren,t,around,as,aside,ask,asking,associated,at,available,away,awfully,be,became,because,become,becomes,becoming,been,before,beforehand,behind,being,believe,below,beside,besides,best,better,between,beyond,both,brief,but,by,c,mon,c,s,came,can,can,t,cannot,cant,cause,causes,certain,certainly,changes,clearly,co,com,come,comes,concerning,consequently,consider,considering,contain,containing,contains,corresponding,could,couldn,t,course,currently,definitely,described,despite,did,didn,t,different,do,does,doesn,t,doing,don,t,done,down,downwards,during,each,edu,eg,eight,either,else,elsewhere,enough,entirely,especially,et,etc,even,ever,every,everybody,everyone,everything,everywhere,ex,exactly,example,except,far,few,fifth,first,five,followed,following,follows,for,former,formerly,forth,four,from,further,furthermore,get,gets,getting,given,gives,go,goes,going,gone,got,gotten,greetings,had,hadn,t,happens,hardly,has,hasn,t,have,haven,t,having,he,he,s,hello,help,hence,her,here,here,s,hereafter,hereby,herein,hereupon,hers,herself,hi,him,himself,his,hither,hopefully,how,howbeit,however,i,d,i,ll,i,m,i,ve,ie,if,ignored,immediate,in,inasmuch,inc,indeed,indicate,indicated,indicates,inner,insofar,instead,into,inward,is,isn,t,it,it,d,it,ll,it,s,its,itself,just,keep,keeps,kept,know,knows,known,last,lately,later,latter,latterly,least,less,lest,let,let,s,like,liked,likely,little,look,looking,looks,ltd,mainly,many,may,maybe,me,mean,meanwhile,merely,might,more,moreover,most,mostly,much,must,my,myself,name,namely,nd,near,nearly,necessary,need,needs,neither,never,nevertheless,new,next,nine,no,nobody,non,none,noone,nor,normally,not,nothing,novel,now,nowhere,obviously,of,off,often,oh,ok,okay,old,on,once,one,ones,only,onto,or,other,others,otherwise,ought,our,ours,ourselves,out,outside,over,overall,own,particular,particularly,per,perhaps,placed,please,plus,possible,presumably,probably,provides,que,quite,qv,rather,rd,re,really,reasonably,regarding,regardless,regards,relatively,respectively,right,said,same,saw,say,saying,says,second,secondly,see,seeing,seem,seemed,seeming,seems,seen,self,selves,sensible,sent,serious,seriously,seven,several,shall,she,should,shouldn,t,since,six,so,some,somebody,somehow,someone,something,sometime,sometimes,somewhat,somewhere,soon,sorry,specified,specify,specifying,still,sub,such,sup,sure,t,s,take,taken,tell,tends,th,than,thank,thanks,thanx,that,that,s,thats,the,their,theirs,them,themselves,then,thence,there,there,s,thereafter,thereby,therefore,therein,theres,thereupon,these,they,they,d,they,ll,they,re,they,ve,think,third,this,thorough,thoroughly,those,though,three,through,throughout,thru,thus,to,together,too,took,toward,towards,tried,tries,truly,try,trying,twice,two,un,under,unfortunately,unless,unlikely,until,unto,up,upon,us,use,used,useful,uses,using,usually,value,various,very,via,viz,vs,want,wants,was,wasn,t,way,we,we,d,we,ll,we,re,we,ve,welcome,well,went,were,weren,t,what,what,s,whatever,when,whence,whenever,where,where,s,whereafter,whereas,whereby,wherein,whereupon,wherever,whether,which,while,whither,who,who,s,whoever,whole,whom,whose,why,will,willing,wish,with,within,without,won,t,wonder,would,would,wouldn,t,yes,yet,you,you,d,you,ll,you,re,you,ve,your,yours,yourself,yourselves,zero,the,of,and,to,is,that,was,for,it,in,om,it,i,it,that,is,in,and,of,to,a,the\".split(',');\n",
        "\n",
        "# se convierten a conjunto\n",
        "stopwords_list = set(stopwords_list)\n",
        "\n",
        "# casos específicos\n",
        "recover_list = {\"wa\":\"was\", \"ha\":\"has\"}\n",
        "\n",
        "def is_stopword(w):\n",
        "    return w in stopwords_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3BA8q6C_ggi",
        "colab_type": "text"
      },
      "source": [
        "### Lematización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIobL654_ggj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wl = nltk.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(w0):\n",
        "    w = wl.lemmatize(w0.lower())\n",
        "    if w in recover_list: return recover_list[w]\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5W3_QkI_ggn",
        "colab_type": "text"
      },
      "source": [
        "## Clase *Vocabulary*\n",
        "\n",
        "Esta clase toma la lista de documentos y extrae las palabras. A cada palabra le asigna un ID. Dependiendo de los stopwords, no todas las palabras de los documentos se retienen para el análisis de los temas.\n",
        "\n",
        "Lo más importante aquí es _self.vocas_ que es la lista de todas aquellas palabras retenidas (que no son stopwords).\n",
        "\n",
        "Los métodos _term_to_id_ y _doc_to_id_ eliminan las palabras repetidas, de tal manera que cada palabra sólo se cuenta cuántas veces aparece en el documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMux6S39_ggp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, excluds_stopwords=False):\n",
        "        self.vocas = []        # id para palabra\n",
        "        self.vocas_id = dict() # palabra que corresponde a un id\n",
        "        self.docfreq = []      # id para la frecuencia del documento\n",
        "        self.excluds_stopwords = excluds_stopwords\n",
        "\n",
        "    def term_to_id(self, term0):\n",
        "        term = lemmatize(term0)\n",
        "        if not re.match(r'[a-z]+$', term): return None\n",
        "        if self.excluds_stopwords and is_stopword(term): return None\n",
        "        try:  \n",
        "            term_id = self.vocas_id[term]\n",
        "        except:\n",
        "            term_id = len(self.vocas)\n",
        "            self.vocas_id[term] = term_id\n",
        "            self.vocas.append(term)\n",
        "            self.docfreq.append(0)\n",
        "        return term_id\n",
        "\n",
        "    def doc_to_ids(self, doc):\n",
        "        l = []\n",
        "        words = dict()\n",
        "        for term in doc.split():\n",
        "            id = self.term_to_id(term)\n",
        "            if id != None:\n",
        "                l.append(id)\n",
        "                if not id in words:\n",
        "                    words[id] = 1\n",
        "                    self.docfreq[id] += 1 \n",
        "                    #Cuenta en cuántos documentos aparece una palabra.\n",
        "                    #Si aparece muy poco la remueve del vocabulario mediante cut_low_freq()\n",
        "        if \"close\" in dir(doc): doc.close()\n",
        "        return l\n",
        "\n",
        "    def cut_low_freq(self, corpus, threshold=1):\n",
        "        new_vocas = []\n",
        "        new_docfreq = []\n",
        "        self.vocas_id = dict()\n",
        "        conv_map = dict()\n",
        "        for id, term in enumerate(self.vocas):\n",
        "            freq = self.docfreq[id]\n",
        "            if freq > threshold:\n",
        "                new_id = len(new_vocas)\n",
        "                self.vocas_id[term] = new_id\n",
        "                new_vocas.append(term)\n",
        "                new_docfreq.append(freq)\n",
        "                conv_map[id] = new_id\n",
        "        self.vocas = new_vocas\n",
        "        self.docfreq = new_docfreq\n",
        "\n",
        "        def conv(doc):\n",
        "            new_doc = []\n",
        "            for id in doc:\n",
        "                if id in conv_map: new_doc.append(conv_map[id])\n",
        "            return new_doc\n",
        "        return [conv(doc) for doc in corpus]\n",
        "\n",
        "    def __getitem__(self, v):\n",
        "        return self.vocas[v]\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.vocas)\n",
        "\n",
        "    def is_stopword_id(self, id):\n",
        "        return self.vocas[id] in stopwords_list\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XExhyeY4_ggr",
        "colab_type": "text"
      },
      "source": [
        "## Clase *HDP*\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--28VZsq_ggr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy, codecs\n",
        "from datetime import datetime\n",
        "from numpy.random import choice\n",
        "from numpy import *\n",
        "import functools\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "sys.setrecursionlimit(10000)\n",
        "\n",
        "import nltk\n",
        "#nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpHfwfru_ggu",
        "colab_type": "text"
      },
      "source": [
        "### Clase _HDP_gibbs_sampling_\n",
        "\n",
        "En el método __init__ method se definen todos los parámetros necesarios:.\n",
        "- Número de temas a obtener.\n",
        "- Los parámetros de las distribuciones alpha, beta y gamma (por default 0.5, 0.5 y 1.5 respectivamente) \n",
        "- La lista de documentos a analizar\n",
        "- El vocabulario que se empleará\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feUCzidx_ggw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDP_gibbs_sampling:    \n",
        "    def __init__(self, K0=10, alpha=0.5, beta=0.5,gamma=1.5, docs= None, V= None):\n",
        "        self.maxnn = 1\n",
        "\n",
        "        # el número de grupos los infiere mediante el número de Stirling de segundo orden\n",
        "        self.alss=[] # guarda el número stirling (N,1:N) para consultarlo\n",
        "        self.K = K0  # el número inicial de temas\n",
        "        \n",
        "        self.alpha = alpha # los temas principales\n",
        "        self.beta = beta   # las palabras principales\n",
        "        self.gamma = gamma # las tablas principales\n",
        "        \n",
        "        self.docs = docs # lista de documentos que incluyen a las palabras\n",
        "        self.V = V # el número de palabras diferentes en el vocabulario\n",
        "        \n",
        "        self.z_m_n = {} # asignación de temas a documentos\n",
        "\n",
        "        # número de temas asignados al tema z en el documento m\n",
        "        self.n_m_z = numpy.zeros((len(self.docs), self.K)) \n",
        "        self.theta = numpy.zeros((len(self.docs), self.K))\n",
        "        \n",
        "        # número de veces que una palabra v se asigna a un tema z\n",
        "        self.n_z_t = numpy.zeros((self.K, V)) \n",
        "        self.phi = numpy.zeros((self.K, V))\n",
        "        \n",
        "        # número total de palabras asignadas a un tema z\n",
        "        self.n_z = numpy.zeros(self.K)   \n",
        "        self.U1=[] # temas activos\n",
        "        for i in range (self.K):\n",
        "            self.U1.append(i)\n",
        "        \n",
        "        self.U0=[] # temas desactivados\n",
        "        self.tau=numpy.zeros(self.K+1) +1./self.K\n",
        "        for m, doc in enumerate(docs):    # initialización de estructuras de datos\n",
        "            for n,t in enumerate(doc):\n",
        "                #aleatoriamente se asigna un tema a una palabra y se incrementa el array de conteo\n",
        "                z = numpy.random.randint(0, self.K) \n",
        "                self.n_m_z[m, z] += 1\n",
        "                self.n_z_t[z, t] += 1\n",
        "                self.n_z[z] += 1\n",
        "                self.z_m_n[(m,n)]=z\n",
        "        \n",
        "    def inference(self,iteration):\n",
        "        \"Inferencia de HDP empleando asignación Direchlet con simplificación ILDA\"\n",
        "        \n",
        "        for m, doc in enumerate(self.docs):\n",
        "            for n, t in enumerate(doc):\n",
        "          \n",
        "            # se decrementa el conteo de la palabra t para el tema k_old\n",
        "                k_old =self.z_m_n[(m,n)]\n",
        "                self.n_m_z[m,k_old] -= 1\n",
        "                self.n_z_t[k_old, t] -= 1\n",
        "                self.n_z[k_old] -= 1\n",
        "\n",
        "                p_z=numpy.zeros(self.K+1)\n",
        "                # se hace muestreo con la función z de ILDA\n",
        "                for kk in range (self.K): \n",
        "                    k=self.U1[kk]\n",
        "                    p_z[kk]=(self.n_m_z[m,k] + self.alpha * self.tau[k])*(self.n_z_t[k,t]+self.beta)/(self.n_z[k]+self.V*self.beta)\n",
        "\n",
        "                # se genera una nueva coordenada para un tema nuevo\n",
        "                p_z[self.K]=(self.alpha*self.tau[self.K])/self.V \n",
        "\n",
        "                k_new = numpy.random.multinomial(1, p_z / p_z.sum()).argmax()\n",
        "\n",
        "                if k_new==self.K:\n",
        "                    # se incrementa el número de temas y arrays y se asigna al array un nuevo tema\n",
        "\n",
        "                    self.z_m_n[(m,n)] = self.spawntopic(m,t) \n",
        "                    # se actualiza la tabla de distribución sobre temas\n",
        "                    self.updatetau() \n",
        "\n",
        "                else :\n",
        "                    # hace lo mismo que LDA\n",
        "                    k=self.U1[k_new] \n",
        "                    self.z_m_n[(m,n)] = k\n",
        "                    self.n_m_z[m,k] += 1\n",
        "                    self.n_z_t[k, t] += 1\n",
        "                    self.n_z[k] += 1\n",
        "                \n",
        "                # verifica si un tema no ha sido usado y reformula sus elementos\n",
        "                if self.n_z[k_old]==0: \n",
        "                    self.U1.remove(k_old)\n",
        "                    self.U0.append(k_old)\n",
        "                    self.K -=1                    \n",
        "                    self.updatetau()\n",
        "\n",
        "        # print ('Iteration:',iteration,'\\n','Number of topics:',self.K,'\\n','Temas activos:',self.U1,'\\n','Deactivated topics',self.U0)\n",
        "\n",
        "\n",
        "    def spawntopic (self,m,t): # reformula los arrays de elementos para un nuevo tema\n",
        "        if len(self.U0)>0: # si hay temas desactivados\n",
        "            k=self.U0[0]\n",
        "            self.U0.remove(k)\n",
        "            self.U1.append(k)\n",
        "            self.n_m_z[m,k]=1\n",
        "            self.n_z_t[k,t]=1\n",
        "            self.n_z[k]=1\n",
        "            \n",
        "            \n",
        "        else:\n",
        "            k=self.K # si hasta el momento no hay temas inactivos\n",
        "            self.n_m_z=numpy.append(self.n_m_z,numpy.zeros([len(self.docs),1]),1)\n",
        "            self.U1.append(k)\n",
        "            self.n_m_z[m,k] = 1\n",
        "            self.n_z_t=numpy.vstack([self.n_z_t,numpy.zeros(self.V)])\n",
        "            self.n_z_t[k, t] = 1\n",
        "            self.n_z=numpy.append(self.n_z,1)\n",
        "            self.tau=numpy.append(self.tau,0)\n",
        "        \n",
        "        self.K +=1\n",
        "        \n",
        "        return k\n",
        "    \n",
        "    def stirling(self,nn): # guardar en una matriz los valores stirling (N, 1: N), única vez\n",
        "        if len(self.alss)==0: \n",
        "            self.alss.append([])\n",
        "            self.alss[0].append(1)\n",
        "        if nn > self.maxnn:\n",
        "            for mm in range (self.maxnn,nn):\n",
        "                ln=len(self.alss[mm-1])+1\n",
        "                self.alss.append([])\n",
        "                \n",
        "                for xx in range(ln) :\n",
        "                    self.alss[mm].append(0)\n",
        "                    if xx< (ln-1):\n",
        "                        self.alss[mm][xx] += self.alss[mm-1][xx]*mm\n",
        "                    if xx>(ln-2) :\n",
        "                        self.alss[mm][xx] += 0\n",
        "                    if xx==0 :\n",
        "                        self.alss[mm][xx] += 0\n",
        "                    if xx!=0 :\n",
        "                        self.alss[mm][xx] += self.alss[mm-1][xx-1]\n",
        "\n",
        "            self.maxnn=nn\n",
        "        return self.alss[nn-1]\n",
        "    \n",
        "    \n",
        "    \n",
        "    def rand_antoniak(self,alpha, n):\n",
        "        # Realiza muestreo mediante una distribution Antoniak\n",
        "        ss = self.stirling(n)\n",
        "        max_val = max(ss)\n",
        "        p = numpy.array(ss) / max_val\n",
        "        \n",
        "        aa = 1\n",
        "        for i, _ in enumerate(p):\n",
        "            p[i] *= aa\n",
        "            aa *= alpha\n",
        "        \n",
        "        p = numpy.array(p,dtype='float') / numpy.array(p,dtype='float').sum()\n",
        "        return choice(range(1, n+1), p=p)\n",
        "    \n",
        "    def updatetau(self):  # actualiza tau mediante muestreo Antoniak\n",
        "    \n",
        "        m_k=numpy.zeros(self.K+1)\n",
        "        for kk in range(self.K):\n",
        "            k=self.U1[kk]\n",
        "            for m in range(len(self.docs)):\n",
        "                \n",
        "                if self.n_m_z[m,k]>1 :\n",
        "                    m_k[kk]+=self.rand_antoniak(self.alpha*self.tau[k], int(self.n_m_z[m,k]))\n",
        "                else :\n",
        "                    m_k[kk]+=self.n_m_z[m,k]\n",
        "    \n",
        "        T=sum(m_k)\n",
        "        m_k[self.K]=self.gamma\n",
        "        tt=numpy.transpose(numpy.random.dirichlet(m_k, 1))\n",
        "        for kk in range(self.K):\n",
        "            k=self.U1[kk]\n",
        "            self.tau[k]=tt[kk]\n",
        "\n",
        "        self.tau[self.K]=tt[self.K]\n",
        "\n",
        "\n",
        "\n",
        "    def worddist(self):\n",
        "        \"\"\"Distribution tema-palabra, \\phi en paper original de Blei  \"\"\"\n",
        "        return (self.n_z_t +self.beta)/ (self.n_z[:, numpy.newaxis]+self.V*self.beta),len(self.n_z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxJQ15jD_gg0",
        "colab_type": "text"
      },
      "source": [
        "### Función Principal\n",
        "\n",
        "Se leen los archivos y se ponen en la lista _corpus_. Una vez realizadas las iteraciones se hacen todas las inferencias para obtener el número de temas _K0_.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z08S4nw1_gg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_doc_content(content_list):\n",
        "    content_string = \"\"\n",
        "    for content in content_list:\n",
        "        content_string = content_string + \" \" + str(content)\n",
        "\n",
        "    return content_string\n",
        "\n",
        "# se invoca el funcionamiento del programa que aplica HDP al corpus\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # iterar los archivos en la carpeta\n",
        "    datasets_dir = \"dataset/bbc/bbc/entertainment\"\n",
        "    \n",
        "    abs_datasets_dir = os.path.abspath(datasets_dir)\n",
        "\n",
        "    filenames = os.listdir(\"dataset/bbc/bbc/entertainment/\")\n",
        "\n",
        "    corpus = list()\n",
        "    for filename in filenames:\n",
        "        fname = abs_datasets_dir + \"/\" + filename\n",
        "        if(filename == \".ipynb_checkpoints\"):\n",
        "            continue\n",
        "        content = get_doc_content(codecs.open(fname , 'r', encoding='utf8').read().splitlines()) \n",
        "        corpus.append(content)\n",
        "\n",
        "    iterations = 100 # el número de iteraciones para que converja el algoritmo\n",
        "    voca = Vocabulary(excluds_stopwords=True) # encuentra las palabras únicas del corpus\n",
        "    docs = [voca.doc_to_ids(doc) for doc in corpus] # cambia las palabras por sus identificadores\n",
        "    HDP = HDP_gibbs_sampling(K0=20, alpha=0.5, beta=0.5, gamma=2, docs=docs, V=voca.size()) # inicializa HDP\n",
        "    for i in range(iterations):\n",
        "        HDP.inference(i)\n",
        "    (d,len) = HDP.worddist() # encuentra la distribución de palabras por cada tema\n",
        "    for i in range(len):\n",
        "        ind = numpy.argpartition(d[i], -10)[-10:] # las 10 palabras top por cada tema\n",
        "        for j in ind:\n",
        "            print (voca[j],' ',end=\"\"),\n",
        "        print ()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YODWycnQ_gg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}